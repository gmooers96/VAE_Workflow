{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set up and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I keep the very basic framework of code in github. When sharing with collaborators, I explain I work in my /project/ directory. The framework can be immidiately cloned from my github page as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /expanse/lustre/projects/sio134/gmooers/\n",
    "git clone https://github.com/gmooers96/VAE_Workflow.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer to use my own environments rather than load modules. I generally first install miniconda:\n",
    "\n",
    "(You can download the .sh file for it here https://docs.conda.io/en/latest/miniconda.html). Then in the command line simply put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once miniconda is installed, I set up my environments.\n",
    "\n",
    "In this cloned repo there are two environments that should have all the packages you need to both train the neural network and do any post-processing. the first one:\n",
    "\n",
    "MOOERS_GPU_ENV.yml\n",
    "\n",
    "can be used to train the model. The cpu environment can be used for post-processing. You can set the environments up like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda env create -f MOOERS_CPU_ENV.yml -n CPU\n",
    "conda env create -f MOOERS_GPU_ENV.yml -n GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each of these can take ~1 hour to set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can try out the neural networks. This repository contains two neural networks the Pritchard Group uses often for our research. The first is a single channel VAE. The files for it are\n",
    "\n",
    "- train_fully_conv.py\n",
    "- model_config/config_1.json\n",
    "- Bash_Scripts/vae_1.sh\n",
    "- sample_fully_conv_improved.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location of the training data can be found in the config file (model_config/config_1.json. More specifically, you should care about lines 8-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"data\": {\n",
    "        \"training_data_path\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Big_Randomized_Trackable/Multi_Sim_Randomized_Space_Time_W_Training.npy\",\n",
    "        \"test_data_path\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Big_Randomized_Trackable/Multi_Sim_Randomized_Space_Time_W_Test.npy\",\n",
    "        \"train_labels\": \"/fast/gmooers/Preprocessed_Data/Centered_50_50/Y_Train.npy\",\n",
    "        \"test_labels\": \"/fast/gmooers/Preprocessed_Data/Centered_50_50/Improved_Y_Test.npy\",\n",
    "        \"max_scalar\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Big_Randomized_Trackable/Multi_Sim_Randomized_Space_Time_Max_Scalar.npy\",\n",
    "        \"min_scalar\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Big_Randomized_Trackable/Multi_Sim_Randomized_Space_Time_Min_Scalar.npy\"\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential are the: \n",
    "- training_data_path\n",
    "- test_data_path\n",
    "- max_scalar\n",
    "- min_scalar\n",
    "\n",
    "But the data is all scaled and ready to go. In the config file you can adjust any hyperparameters (batch size, learning rate, filter size, ect..) and it will be automatically read into the training file. You can either launch the model from the command line like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate {name of the GPU Environment}\n",
    "python3 train_fully_conv.py --id 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But given the VAE takes hundreds of epochs (typically over 24 hours) to train, I usually rely on the Expanse queue. You can submit the model immidiately to the queue via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Bash_Scripts\n",
    "sbatch vae_1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though you may need to change line 22 depending on what you name your GPU environment (default below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source activate GPU2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of analysis, the VAE archetexture you specify in the config file will auotmatically send a diagrma of the VAE encoder and decoder to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graphs/model_diagrams/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, upon successful completion of training, the loss curves will be saved to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graphs/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience you want to see a reconstruction learning curve immidiately minimzing, finding a minima, and the validation loss curve overfitting after several hundred epochs (the code will save the best model, so this is not a problem)\n",
    "\n",
    "On the otherhand, since this is a (linearly) annealling VAE, the KL Divergence will spike up for the first several epochs, the begin to minimize as we weight in the term in the loss function more with each passing epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analysis of the model can be done using the sampling script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 sample_fully_conv_improved.py --id 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit of a clunky script, with some residual hardcoding I have never had the time to correct. Vasically there are two built in function you can use to analyze the trained VAE. On line 594, comment in this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_latent_space_var(encoder_result.vae_encoder, train_data, test_data, args.id, dataset_min, dataset_max,  args.dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the sampling script is now run, it will create a visualization of the latent space (you can hard code in the final diensionality within the function itself, I reccomend two or three). In my experience, this has been the best way to tell if (for representation learning) our VAE is successfully trained.\n",
    "\n",
    "Another option is to comment in line 593 and visualize reconstrctions of specific vertical velocity fields fro mthe VAE Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_targets_paper(vae, test_data, [2, 15, 66 , 85, 94], args.id, dataset_max, dataset_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other model is the multichannel VAE. the procedure to train and analyze it should be almost idential to above.\n",
    "\n",
    "- train_fully_conv_multichannel.py\n",
    "- model_config/config_3.json\n",
    "- Bash_Scripts/vae_3.sh\n",
    "- sample_fully_conv_improved_multichannel.py\n",
    "\n",
    "But note in the config file, it is pulling from three variables (vertical velocity again but also temperature and water vapor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"data\": {\n",
    "        \"training_data_path\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Centered_50_50/Space_Time_W_Training.npy\",\n",
    "\n",
    "        \n",
    "        \"training_data_path_T\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/T_Variable/Space_Time_Anon_T_Training.npy\",\n",
    "        \n",
    "        \"training_data_path_Q\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/Q_Variable/Space_Time_Anon_Q_Training.npy\",\n",
    "        \n",
    "        \"training_data_path_W\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/W_Variable/Space_Time_W_Training.npy\",\n",
    "        \n",
    "        \"test_data_path_T\" : \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/T_Variable/Space_Time_Anon_T_Test.npy\",\n",
    "        \n",
    "        \"test_data_path_Q\" : \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/Q_Variable/Space_Time_Anon_Q_Test.npy\",\n",
    "        \n",
    "         \"test_data_path_W\" : \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/W_Variable/Space_Time_W_Test.npy\",\n",
    "        \n",
    "        \"train_labels\": \"/Preprocessed_Data/Centered_50_50/Y_Train.npy\",\n",
    "        \"test_labels\": \"/Preprocessed_Data/Centered_50_50/Improved_Y_Test.npy\",\n",
    "        \"max_scalar_t\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/T_Variable/Space_Time_Anon_Max_Scalar.npy\",\n",
    "        \"min_scalar_t\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/T_Variable/Space_Time_Anon_Min_Scalar.npy\",\n",
    "         \"max_scalar_q\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/Q_Variable/Space_Time_Anon_Max_Scalar.npy\",\n",
    "        \"min_scalar_q\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/Q_Variable/Space_Time_Anon_Min_Scalar.npy\",\n",
    "         \"max_scalar_w\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/W_Variable/Space_Time_Max_Scalar.npy\",\n",
    "        \"min_scalar_w\": \"/expanse/lustre/projects/sio134/gmooers/CBRAIN-CAM/MAPS/Preprocessed_Data/Trackable_Data/W_Variable/Space_Time_Min_Scalar.npy\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also take much longer (but less epochs) to train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
